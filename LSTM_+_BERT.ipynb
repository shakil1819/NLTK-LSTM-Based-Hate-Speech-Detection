{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmuEXEVV2SlJiY2p0BBlQz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shakil1819/NLTK-LSTM-Based-Hate-Speech-Detection/blob/main/LSTM_%2B_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IMxgpdn23_Q",
        "outputId": "333c637a-4047-45f7-bc43-30b7bafed32b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tweet    0\n",
            "class    0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-b5995b0ecfa6>:68: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  df['tweet'] = df['tweet'].str.replace(punct_sign, '')\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "<ipython-input-1-b5995b0ecfa6>:84: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['tweet'] = df['tweet'].str.replace(regex_stopword, '')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy is:  0.8407531943510423\n",
            "accuracy is:  0.8221923335574983\n",
            "accuracy is:  0.8468056489576328\n",
            "Epoch 1/5\n",
            " 28/272 [==>...........................] - ETA: 2:39 - loss: 0.5728 - accuracy: 0.7606"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"LSTM_Twitter_dataset.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/github/shakil1819/NLTK-LSTM-Based-Hate-Speech-Detection/blob/main/LSTM_Twitter_dataset.ipynb\n",
        "\n",
        "# Importing Libraries\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import nltk\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import itertools\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Dense,LSTM, SpatialDropout1D, Embedding\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from joblib import dump, load\n",
        "\n",
        "\"\"\"# Reading the dataset\"\"\"\n",
        "\n",
        "text = []\n",
        "clas = []\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/shakil1819/NLTK-LSTM-Based-Hate-Speech-Detection/main/Dataset/labeled_data.csv')\n",
        "text = df['tweet'].tolist()\n",
        "clas = df['class'].tolist()\n",
        "df.head()\n",
        "\n",
        "\"\"\"# creating a new dataframe for easy text processing\"\"\"\n",
        "\n",
        "df = pd.DataFrame({'tweet': text, 'class': clas})\n",
        "\n",
        "\"\"\"# Finding if there is any missing data\"\"\"\n",
        "\n",
        "print(df.isnull().sum())\n",
        "\n",
        "\"\"\"# Converting the data into lower case.\"\"\"\n",
        "\n",
        "df['tweet'] = df['tweet'].apply(lambda x:x.lower())\n",
        "\n",
        "\"\"\"# removing punctuations\"\"\"\n",
        "\n",
        "punctuation_signs = list(\"?:!.,;\")\n",
        "df['tweet'] = df['tweet']\n",
        "\n",
        "for punct_sign in punctuation_signs:\n",
        "    df['tweet'] = df['tweet'].str.replace(punct_sign, '')\n",
        "\n",
        "\"\"\"# Removing '\\n' and '\\t', extra spaces, quoting text, and progressive pronouns.\"\"\"\n",
        "\n",
        "df['tweet'] = df['tweet'].apply(lambda x: x.replace('\\n', ' '))\n",
        "df['tweet'] = df['tweet'].apply(lambda x: x.replace('\\t', ' '))\n",
        "df['tweet'] = df['tweet'].str.replace(\"    \", \" \")\n",
        "df['tweet'] = df['tweet'].str.replace('\"', '')\n",
        "df['tweet'] = df['tweet'].str.replace(\"'s\", \"\")\n",
        "\n",
        "\"\"\"# removing stop-words\"\"\"\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = list(stopwords.words('english'))\n",
        "for stop_word in stop_words:\n",
        "    regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
        "    df['tweet'] = df['tweet'].str.replace(regex_stopword, '')\n",
        "\n",
        "\"\"\"# Using Bag of Words approach for final data Preparation.Â¶\"\"\"\n",
        "\n",
        "cv = CountVectorizer(max_features = 75)\n",
        "X = cv.fit_transform(df['tweet']).toarray()\n",
        "y = df['class']\n",
        "\n",
        "\"\"\"# Splitting the Data using Stratified split\"\"\"\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify=y, random_state = 42)\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                              cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "\"\"\"# Using Random Forest Classifier as the Model and printing evaluating it using confusion matrix\"\"\"\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=10)\n",
        "clf = clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy is: \",accuracy)\n",
        "CM = confusion_matrix(y_test, y_pred)\n",
        "plot_confusion_matrix(CM, classes = range(3))\n",
        "dump(clf, 'rf.joblib')\n",
        "\n",
        "\"\"\"# Using Decision tree as the Model and printing evaluating it using confusion matrix\"\"\"\n",
        "\n",
        "clf = tree.DecisionTreeClassifier()\n",
        "clf = clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy is: \",accuracy)\n",
        "CM = confusion_matrix(y_test, y_pred)\n",
        "plot_confusion_matrix(CM, classes = range(3))\n",
        "dump(clf, 'decision.joblib')\n",
        "\n",
        "\"\"\"# Using AdaBoost Classifier as the Model and printing evaluating it using confusion matrix\"\"\"\n",
        "\n",
        "clf = AdaBoostClassifier(n_estimators=100)\n",
        "clf = clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy is: \",accuracy)\n",
        "CM = confusion_matrix(y_test, y_pred)\n",
        "plot_confusion_matrix(CM, classes = range(3))\n",
        "dump(clf, 'ada.joblib')\n",
        "\n",
        "\"\"\"# Converting the labels into categorical format\"\"\"\n",
        "\n",
        "y_train=to_categorical(y_train, num_classes = 3, dtype='float32')\n",
        "y_test=to_categorical(y_test, num_classes = 3, dtype='float32')\n",
        "\n",
        "\"\"\"# Creating and Training an LSTM Model\"\"\"\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(232337, 100, input_length=X_train.shape[1]))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(20, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 64\n",
        "\n",
        "history = model.fit(X_train, y_train,validation_data = (X_test,y_test), epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\"\"\"# Saving the LSTM Model\"\"\"\n",
        "\n",
        "model.save('lstm.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Incorporating bert embeddings"
      ],
      "metadata": {
        "id": "q6hyQuvg3G9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import BERT tokenizer and embeddings\n",
        "#!pip install pytorch-pretrained-bert\n",
        "!pip install transformers\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load pretrained BERT\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize tweets using BERT tokenizer\n",
        "encoded_tweets = [bert_tokenizer.encode_plus(tweet,\n",
        "                   max_length=128,\n",
        "                   pad_to_max_length=True)\n",
        "                 for tweet in df['tweet'].values]\n",
        "\n",
        "import torch\n",
        "tweet_tokens = {'input_ids': torch.tensor([x['input_ids'] for x in encoded_tweets]),\n",
        "                'attention_mask': torch.tensor([x['attention_mask'] for x in encoded_tweets])}\n",
        "# Extract BERT embeddings for tokens\n",
        "tweet_embeddings = bert(tweet_tokens['input_ids'])['last_hidden_state']\n",
        "\n",
        "# Build model input using tweet BERT embeddings\n",
        "tweet_input = Input(shape=(MAX_LEN, 768), dtype='float32')\n",
        "\n",
        "# Pass tweet embeddings to LSTM\n",
        "x = LSTM(64)(tweet_embeddings)\n",
        "\n",
        "# Rest of the model same as before\n",
        "x = Dense(32, activation='relu')(x)\n",
        "out = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model = Model(tweet_input, out)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "# Train model on tweet embeddings same way\n",
        "model.fit(tweet_embeddings, y_train,\n",
        "          epochs=10, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49qRgQ5O3FnY",
        "outputId": "ff8cf0f9-0983-4761-ea83-9b3a21dfc979"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "hZDo2ojF3Lvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model evaluation\n",
        "y_pred = model.predict(X_test_bert)\n",
        "\n",
        "# Accuracy\n",
        "acc = accuracy_score(y_test, y_pred.argmax(1))\n",
        "print(\"Test Accuracy:\", acc)\n",
        "\n",
        "# Loss and Accuracy plots\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['acc'])\n",
        "plt.title('Model loss and accuracy')\n",
        "plt.ylabel('Loss/Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Loss', 'Accuracy'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred.argmax(1))\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_test, y_pred.argmax(1)))\n",
        "\n",
        "# ROC Curve\n",
        "plt.title('ROC Curve')\n",
        "plot_roc_curve(model, X_test_bert, y_test)"
      ],
      "metadata": {
        "id": "z1akZsmP3Nk2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}